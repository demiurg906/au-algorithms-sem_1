# Другие модели вычислений

Есть много разных модлей вычислений:

* **RAM-машина:** классическая модель вычислений, которую мы использовали до сих пор.
* **Модель внешней памяти (cache-aware):** модель вычислений, в которой есть ограниченная оперативная память и более-менее безграничная внешняя память (жесткий диск, магнитные лента), с очень медленной скоростью доступа к ней (на несколько порядков меньше, чем у оперативной).

    Ключевым узким местом в данной модели являются операции чтения из внешней памяти, алгоритмы в оперативке считаются эффективными, и на их время работы можно забить.
* **Cache-oblivious:** эта модель схожа с моделью внешней памяти, но ничего не знаем про количество доступной памяти.
* **Распределенные вычисления:** есть множество машин с какой-то моедлью вычислений, которые могут общаться друг с другом по сети
* **Квантовые комьютеры**
* **Вычисления на GPU**
* **Streaming алгоритмы:** есть устройства с ограниченной памятью и вычислительной возможностью, необходимо работать с большими данными (роутер)

## Модель внешней памяти

Есть процессор и RAM размера $M$, с которой он может работать. Также есть внешняя память и некоторый IO контроллер, который может выгружать данные из внешней памяти в RAM блоками размера $B$.

Имеется два параметра -- $M$ И $B$, интересует количество IO операций.

### Всякие алгоритмы

#### Поиск в массиве

Есть массив размера $N$, который линейно лежит в памяти, необходимо найти определнный элемент.

Тип поиска:
* Линейный поиск: $O(N / B)$ IO операций.
* Бинарный поиск: $O(\log_2\frac NB)$ IO операций, работаем в $B$ раз хуже, чем в обычной модели.

#### Reverse массива

$O(N / B)$ операций, если в память помещается два блока (читаем два противоположных блока, за $O(1)$ переворачиваем элементы в блоках, записываем их на противоположные места).

### Сортировка

Используем аналог обычного merge-sort. Его можно обозвать 2-way merge-sort, т.к. на вход merge подается два отсортированных куска, и они мерджатся.

Мы же будем использовать $K$-way merge-sort.

Давайте возьмем $K = \Theta\left(\frac MB - 1\right)$ ($-1$ нужен для того, чтобы у нас осталось место для буфера, который мы будем записывать в память). Как работает merger: в $\frac MB - 1$ блоков записываем по одному блоку из каждого отсортированного куска, в последнем блоке у нас буфер, в который мы сливаем массивы. Если буфер переполняется, мы записываем его во внешнюю память, если кончается какой-то блок из подмассива, то мы читаем следующий блок.

Общая схема алгорима:
1. Разбить $A$ на $K$ массивов
2. Рекурсивно отсортировать каждый
3. Слить при помощи $K$-merge

Уровней рекурсии: $O\left(\frac NB \cdot \log_{\frac MB} \frac NB \right)$
Время работы: $T(N) = K \cdot T(N/K) + \Theta(N/B)$

## Модель cache-oblivious

В данной моели неизвестны $M$ и $B$. Также мы не контролируем поведение загруженных блоков в RAM (нет произвольного выделения/освобождения памяти).

Есть CPU и RAM, между ними есть несколько уровней кешей (обычно три для CPU). Если алгоритм $A$ cache-oblivious оптимальный. то он оптимальный для любой иерархии кешей.

Не смотря на все ограничения, алгоритм поиска в массиве из модели с внешней памятью останется оптимальным.

Предположения модели:
1. $M \geq B^2$ -- предположение о высоте кеша, количество элементов в блоке не меньше, чем число блоков в кеше
2. Кеш ассоциативен: если есть кеш с блоками, то в любой блок можно записать любой блок из памяти (в реальных процессорных кешах это не так, там каждый кусочек кеша имеет доступ только к некоторому подмножеству памяти)
3. Стратегия управения кешом оптимальна. Стратегия отвечает на вопрос, что делать, если кеш переполнился:
    - LRU -- Last Resently Used: выкидывается блок, который дольше всего не использовался
    - FIFO: выкидывается самый старый загруженный блок

**Теорема:** если оптимальная стратегия сделает $f$ чтений для кеша размера $M / 2$, то LRU (FIFO) сделает $\leq 2f$ чтений для кеша размера $M$.

**Доп. свойство для алгоритмов (условие регулярности):** $T(B, M / 2) = O(T(B, M))$.

**Утверждение:** Если алгоритм $A$ в предполежении оптимальной стратегии управения кешем $T(B, M)$, то если мы перейдем от оптимальной стратегии к LRU (FIFO), то он будет работать $\Theta(T(B, M))$ (увеличится в константу раз).

**Доказательство теоремы:** рассмотрим последовательность чтений для LRU. Разобьем их на кусочки $s_i$, в которых количество чтений LRU меньше размера кеша $(M/ B)$, $f = M/B - 1$. Оптимальная стратегия считает $f - M/2B = f / 2$ чтений.

### Сортировка

$k$-воронка ($k$-funnel) -- структрура данных, которая сливает $k^3$ ($k$ такое, что $k^3 = B$) отсортированных массивов (списков) за $O\left(\frac{k^3}B \log_{\frac MB}\frac{k^3}B + k\right)$, требует $O(k^2)$ памяти.

Алгоритм:
1. Разбиваем вход на массивы размера $N^{\frac 23}$, получим $N^{frac 13}$ подмассивов
2. Рекурсивно сортируем
3. Сливаем при помощи $k$-воронки

Время работы: $T(N) = N^{\frac 13}\cdot T(N^{\frac 23}) + O\left(\frac{k^3}B \log_{\frac MB}\frac{k^3}B + N^{\frac 13}\right)$, если $N\geq B^2$, то тогда можно выкинуть $N^{\frac 13}$ из $O(...)$, и итоговое время работы будет $T(N) = O\left(\frac{k^3}B \log_{\frac MB}\frac{k^3}B\right)$
